# 学习汇报
## “LOOPY: TAMING AUDIO-DRIVEN PORTRAIT AVATAR WITH LONG-TERM MOTION DEPENDENCY” 论文学习
### 论文概述
1.去除掉传统都在用的**Face Locator和Speed Layer层**，取而代之的是额外的一层片段间自注意力层---**INTER/INTRA- CLIP TEMPORAL LAYERS DESIGN**
2.以往把Audio2Vec后直接进网络计算注意力，该论文将其编码成latents再计算注意力
论文结构图

![](/imgs/2024-11-25/2SIbvTbFGrD9TxZ2.png)
### 出发点
对比视频链接：https://loopyavatar.github.io/video/comparison_1.mp4
![输入图片说明](/imgs/2024-11-25/4XX68LqDqjY7SqVH.png)
![输入图片说明](/imgs/2024-11-25/dWdGz3wzCheXtULR.png)
**观察发现**
Hallo和V-Express头部全程保持一个姿势（一个朝向），这显然不自然，观察其模型结构，确认使用了FaceEncoder（识别出面部框生成mask）和V-kps（左眼、右眼、嘴巴三点定脸的位置），显然没有头部姿态的3D信息，就生成不出来头部具有3D动态的视频。
而Echomimic的做法是用了一个landmark encoder，把眼睛鼻子嘴在2D图像上标注出来，然后随机丢弃改变位置，输入模型以增强适应能力
>在 EchoMimic 框架中，**Landmark Encoder** 是一个关键组件，负责将面部关键点图像编码为latents表示，然后再和多帧噪声相加输入u-net指导去噪。
[CSDN 博客](https://blog.csdn.net/qq_44091004/article/details/141971790?utm_source=chatgpt.com)

所以Loopy不在依靠其他编码器来提取头部姿势位置等的特征，而是直接将训练视频和多帧噪声直接输入，通过两层**INTER/INTRA- CLIP TEMPORAL LAYERS DESIGN**替代了以前姿势特征提取的功能。

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTExNTExMjk5MCwxMzkzNDU2MTEyLDExNj
g1NDQyMjEsMTQ2ODIxODk1MSwtODE0MTIwNDAsMTI4NzEwNjcz
OSwtNzY5NDkxMDAzLDU5MTIxMTY5Ml19
-->