# 学习汇报
## “LOOPY: TAMING AUDIO-DRIVEN PORTRAIT AVATAR WITH LONG-TERM MOTION DEPENDENCY” 论文学习
### 论文概述
1.去除掉传统都在用的**Face Locator和Speed Layer层**，取而代之的是额外的一层片段间自注意力层---**INTER/INTRA- CLIP TEMPORAL LAYERS DESIGN**
2.以往把Audio2Vec后直接进网络计算注意力，该论文将其编码成latents再计算注意力
论文结构图

![](/imgs/2024-11-25/2SIbvTbFGrD9TxZ2.png)
### 出发点
对比视频链接：https://loopyavatar.github.io/video/comparison_1.mp4
![输入图片说明](/imgs/2024-11-25/4XX68LqDqjY7SqVH.png)
![输入图片说明](/imgs/2024-11-25/dWdGz3wzCheXtULR.png)
观察发现
Hallo和V-Express头部全程保持一个姿势（一个朝向），这显然不自然，观察其模型结构，确认使用了FaceEncoder（识别出面部框生成mask）和V-kps（左眼、右眼、嘴巴三点定脸的位置），显然没有头部姿态的3D信息，就生成不出来头部具有3D动态的视频。
而Echomimic的做法是使用一个landmark encoder

-   **编码面部关键点：** Landmark Encoder 接收面部关键点图像作为输入，利用卷积神经网络（CNN）提取特征，将其转换为与潜在空间（latent space）对齐的特征表示。
-   **指导视频生成：** 编码后的特征表示与多帧潜在变量（latents）相加，然后输入到去噪 U-Net（Denoising U-Net），从而在生成过程中引导模型生成符合预期面部姿态和表情的视频帧。
- >在 EchoMimic 框架中，**Landmark Encoder** 是一个关键组件，负责将面部关键点图像编码为特征表示，以指导生成模型合成逼真的人像视频。
[CSDN 博客](https://blog.csdn.net/qq_44091004/article/details/141971790?utm_source=chatgpt.com)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTMxMDgxMzI2NCwtODE0MTIwNDAsMTI4Nz
EwNjczOSwtNzY5NDkxMDAzLDU5MTIxMTY5Ml19
-->